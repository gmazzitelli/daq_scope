{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d9c726d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected LECROY,WR8108HD,LCRY5004N60822,10.5.0\n"
     ]
    }
   ],
   "source": [
    "import TeledyneLeCroyPy\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../.google_credentials.json')\n",
    "scope = ['https://spreadsheets.google.com/feeds']\n",
    "creds_scope = credentials.with_scopes(scope)\n",
    "client = gspread.authorize(creds_scope)\n",
    "\n",
    "sheet = client.open_by_key('1idBnsYG4pHdHZ2kDbq-S3PBUAf0ewFEzKXOtCTGDxyU')\n",
    "log = sheet.worksheet(\"log\")\n",
    "\n",
    "#\n",
    "# data saved in logbook\n",
    "# https://docs.google.com/spreadsheets/d/1idBnsYG4pHdHZ2kDbq-S3PBUAf0ewFEzKXOtCTGDxyU\n",
    "#\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        #o = TeledyneLeCroyPy.LeCroyWaveRunner('TCPIP0::192.168.99.103::inst0::INSTR')\n",
    "        o = TeledyneLeCroyPy.LeCroyWaveRunner('TCPIP0::192.168.189.115::inst0::INSTR')\n",
    "        break\n",
    "    except:\n",
    "        end = time.time()\n",
    "        print(\"waiting for connection... \"+str(int(end-start))+\"s\", end=\"\\r\")\n",
    "        time.sleep(0.1)\n",
    "        pass\n",
    "print(\"Connected\", o.idn) # Prings e.g. LECROY,WAVERUNNER9254M,LCRY4751N40408,9.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8bae4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_record_to_hdf5(filename, record_id, record_data):\n",
    "    with h5py.File(filename, 'a') as hdf_file:\n",
    "        group = hdf_file.create_group(str(record_id))\n",
    "        for key, value in record_data.items():\n",
    "            group.create_dataset(key, data=value)\n",
    "            \n",
    "def append_record_to_hdf5_(filename, record_id, record_data):\n",
    "#\n",
    "# deve essere un array, ma non funziona quindi i record vanno messi fra []. c'e' qulcosa che non ho cpaito \n",
    "# esempio: dict2save['H'+str(channel)]=[json.dumps(data['wavedesc'], default=str)]\n",
    "#\n",
    "    with h5py.File(filename, 'a') as hdf_file:\n",
    "        group = hdf_file.create_group(str(record_id))\n",
    "        for key, value in record_data.items():\n",
    "            group.create_dataset(key, data=value,\n",
    "                      compression='gzip',\n",
    "                      compression_opts=9)\n",
    "            \n",
    "def append_record_to_pickle(filename, record):\n",
    "    # Read existing data from the pickle file\n",
    "    try:\n",
    "        with open(filename, 'rb') as file:\n",
    "            existing_data = pickle.load(file)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = []\n",
    "    # Append the new record to the existing data\n",
    "    existing_data.append(record)\n",
    "\n",
    "    # Write the updated data back to the pickle file\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(existing_data, file)\n",
    "            \n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "from pymemcache.client.base import Client\n",
    "\n",
    "\n",
    "def mc_get_str(ip, key):\n",
    "\n",
    "    client = Client((ip, 11211))\n",
    "    result = client.get(key)\n",
    "    return result.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8e65fd19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run description \n",
      "Number of channels: [4] \n",
      "Number of events: [1000] 10\n",
      "file type: [H5] else PKL\n",
      "File removed: /Volumes/WC/data/run_00004.h5\n",
      "Triggers acquired: 9, elapsed 6.1 s, Tr Hz: 0.7, storing time: 0.49 s\n",
      "Upating metadata run info...\n",
      "\n",
      "Uploding file on cloud in background...\n",
      "closing remarks? (if any [])\n",
      "\n",
      "Updating logbook...\n",
      "\n",
      "RUN run_00004.h5 ACQUIRED\n",
      "\n",
      "DAQ STOP\n"
     ]
    }
   ],
   "source": [
    "EVENTS= 1000\n",
    "DSHOW = False\n",
    "FTYPE = 'H5'\n",
    "CHANNELS = 4\n",
    "PATH = '/Volumes/WC/data/'\n",
    "\n",
    "user_input = input(\"Run description \")\n",
    "if user_input:\n",
    "    start_desc=user_input\n",
    "else:\n",
    "    start_desc=''\n",
    "\n",
    "user_input = input(\"Number of channels: [{}] \".format(CHANNELS)).lower()\n",
    "if user_input:\n",
    "    channels=int(user_input)\n",
    "else:\n",
    "    channels=CHANNELS\n",
    "user_input = input(\"Number of events: [{}] \".format(EVENTS)).lower()\n",
    "if user_input:\n",
    "    events=int(user_input)\n",
    "else:\n",
    "    events=EVENTS\n",
    "\n",
    "user_input = input(\"file type: [{}] else PKL\".format(FTYPE)).lower()\n",
    "if user_input=='pkl':\n",
    "    pkl = True\n",
    "else:\n",
    "    pkl = False\n",
    "\n",
    "#pkl=True\n",
    "\n",
    "# reading logbook\n",
    "logdf=pd.DataFrame.from_dict(log.get_all_records())\n",
    "last_run = logdf.run.values[-1]\n",
    "run = last_run+1\n",
    "    \n",
    "    \n",
    "if pkl:\n",
    "    filename = \"run_{:05d}.pkl\".format(run)\n",
    "else:\n",
    "    filename = \"run_{:05d}.h5\".format(run)\n",
    "filepath = PATH+filename\n",
    "    \n",
    "#########################\n",
    "start = time.time()\n",
    "dt = 0\n",
    "\n",
    "if os.path.exists(filepath):\n",
    "    os.remove(filepath)\n",
    "    print('File removed:', filepath)\n",
    "else:\n",
    "    print('Writing on new file:', filepath)\n",
    "\n",
    "beam = mc_get_str(ip='192.168.198.164', key='BTFDATA_PADME')\n",
    "# updating startup condition\n",
    "start_date = time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.gmtime())\n",
    "start_epoch = time.time()\n",
    "logdf = logdf._append({'run': run, 'start_desc': start_desc, 'start_date': start_date, 'start_epoch': start_epoch, \n",
    "                       'filename': filename, 'beam': beam}, ignore_index=True)\n",
    "\n",
    "\n",
    "for event in range(events):\n",
    "    dict2save = {}\n",
    "    triggers = []\n",
    "    try:\n",
    "        elapsed = time.time()-start\n",
    "        if event >0:\n",
    "            trate = elapsed/event\n",
    "        else:\n",
    "            trate = 0 \n",
    "        print('Triggers acquired: {:d}, elapsed {:.1f} s, Tr Hz: {:.1f}, storing time: {:.2f} s'.format(event,elapsed, trate, dt), end=\"\\r\")\n",
    "        o.wait_for_single_trigger() # Halt the execution until there is a trigger.\n",
    "        dt1 = time.time()\n",
    "        for channel in range(1, channels+1):\n",
    "            data = o.get_waveform(n_channel=channel)\n",
    "            if pkl:\n",
    "                triggers.append(data)\n",
    "            else:\n",
    "                dict2save['H'+str(channel)]=json.dumps(data['wavedesc'], default=str)\n",
    "                dict2save['T'+str(channel)]=json.dumps(data['trigtime'])\n",
    "                dict2save['W'+str(channel)]=json.dumps(data['waveforms'], cls=NumpyEncoder)\n",
    "            if DSHOW:\n",
    "                # show first waveform per \n",
    "                x = data['waveforms'][0]['Time (s)']/1e-9\n",
    "                y = data['waveforms'][0]['Amplitude (V)']/1e-3\n",
    "                plt.plot(x,y, label=\"C\"+str(n_channel))\n",
    "\n",
    "        \n",
    "        if pkl:\n",
    "            append_record_to_pickle(filepath, triggers)\n",
    "        else:\n",
    "            dict2save['epoch']=data['wavedesc']['TRIGGER_TIME'].timestamp()\n",
    "            append_record_to_hdf5(filepath, event, dict2save)\n",
    "        if DSHOW:\n",
    "            t = data['wavedesc']['TRIGGER_TIME'].strftime('%m/%d/%Y %H:%M:%S')\n",
    "            plt.legend()\n",
    "            plt.title(t)\n",
    "            plt.xlabel('ns')\n",
    "            plt.ylabel('mV')\n",
    "            plt.show()\n",
    "        dt =  time.time() - dt1\n",
    "    except KeyboardInterrupt:\n",
    "        print (\"\\nstopping DAQ...\")\n",
    "        break\n",
    "\n",
    "print (\"\\nUpating metadata run info...\")\n",
    "# updating ending condition\n",
    "end_date = time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.gmtime())\n",
    "logdf.at[run, 'end_date']=end_date\n",
    "end_epoch = time.time()\n",
    "logdf.at[run, 'end_epoch']=end_epoch\n",
    "logdf.at[run, 'events']=event+1\n",
    "\n",
    "print (\"\\nUploding file on cloud in background...\")\n",
    "subprocess.Popen(['./uploadFile.py', filepath], stdout=None, stderr=None, stdin=None)\n",
    "\n",
    "user_input = input(\"closing remarks? (if any [])\")\n",
    "if user_input:\n",
    "    end_desc=user_input\n",
    "else:\n",
    "    end_desc=''\n",
    "logdf.at[run, 'end_desc']=end_desc\n",
    "\n",
    "# updating logbook\n",
    "print (\"\\nUpdating logbook...\")\n",
    "log.update([logdf.columns.values.tolist()] + logdf.values.tolist())\n",
    "\n",
    "print(\"\\nRUN {:} ACQUIRED\".format(filename))\n",
    "print(\"\\nDAQ STOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dd18efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymemcache.client.base import Client\n",
    "\n",
    "\n",
    "def mc_get_str(ip, key):\n",
    "\n",
    "    client = Client((ip, 11211))\n",
    "    result = client.get(key)\n",
    "    return result.decode(\"utf-8\")\n",
    "\n",
    "beam = mc_get_str(ip='192.168.198.164', key='BTFDATA_PADME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c085b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"BTFDATA_PADME,3796045233.093758,0,1,0,1,0,0,449.21,0.00,0.00,0,1,1,1,298.00,286.00,0.00\\\\r\\\\nBTFKEY_PADME,KEY_TIM,KEY_STA,DHPTB101_OK,BTF_PHA,DHSTB001_OK,DHSTB002_OK,DHRTB102_OK,DHSTB001_ENE,DHSTB002_ENE,DHRTB102_CUR,LINAC_PHA,MOD_OK,GUN_OK,BTFDAQ_OK,DHRTB101_CUR,DHSTB001_CUR,DHSTB002_CUR\\\\r\\\\n\\\\r\\\\n\"'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7f4526f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Group' object has no attribute 'compression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m hdf_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Verificare le proprietà del dataset\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompression options:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mcompression_opts)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Group' object has no attribute 'compression'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Aprire il file HDF5\n",
    "with h5py.File(filepath, 'r') as hdf_file:\n",
    "    # Ottenere il dataset di interesse\n",
    "    dataset = hdf_file[\"0\"]\n",
    "    \n",
    "    # Verificare le proprietà del dataset\n",
    "    print(\"Compression:\", dataset.compression)\n",
    "    print(\"Compression options:\", dataset.compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c5985297",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict2save.items():\n",
    "    print(key, type([value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "91057f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"DESCRIPTOR_NAME\": \"WAVEDESC\", \"TEMPLATE_NAME\": \"LECROY_2_3\", \"COMM_TYPE\": 1, \"COMM_ORDER\": 0, \"WAVE_DESCRIPTOR\": 346, \"USER_TEXT\": 0, \"RES_DESC1\": 0, \"TRIGTIME_ARRAY\": 0, \"RIS_TIME_ARRAY\": 0, \"RES_ARRAY1\": 0, \"WAVE_ARRAY_1\": 20004, \"WAVE_ARRAY_2\": 0, \"RES_ARRAY2\": 0, \"RES_ARRAY3\": 0, \"INSTRUMENT_NAME\": \"LECROYWaveRunner\", \"INSTRUMENT_NUMBER\": 20543, \"TRACE_LABEL\": \"\", \"RESERVED1\": 0, \"RESERVED2\": 10002, \"WAVE_ARRAY_COUNT\": 10002, \"PNTS_PER_SCREEN\": 10000, \"FIRST_VALID_PNT\": 0, \"LAST_VALID_PNT\": 10001, \"FIRST_POINT\": 0, \"SPARSING_FACTOR\": 1, \"SEGMENT_INDEX\": 0, \"SUBARRAY_COUNT\": 1, \"SWEEPS_PER_ACQ\": 1, \"POINTS_PER_PAIR\": 0, \"PAIR_OFFSET\": 0, \"VERTICAL_GAIN\": 1.2972400327271316e-06, \"VERTICAL_OFFSET\": 0.03420000150799751, \"MAX_VALUE\": 30579.0, \"MIN_VALUE\": -30835.0, \"NOMINAL_BITS\": 8, \"NOM_SUBARRAY_COUNT\": 1, \"HORIZ_INTERVAL\": 5.00000006675716e-11, \"HORIZ_OFFSET\": -3.1404710717026614e-07, \"PIXEL_OFFSET\": -3.14e-07, \"VERTUNIT\": \"V\", \"HORIZ_UNCERTAINTY\": 9.999999960041972e-13, \"TRIGGER_TIME\": \"2024-04-09 20:46:09\", \"ACQ_DURATION\": 0.0, \"RECORD_TYPE\": 0, \"PROCESSING_DONE\": 0, \"RESERVED5\": 0, \"RIS_SWEEPS\": 1, \"TIMEBASE\": 14, \"VERT_COUPLING\": 0, \"PROBE_ATT\": 1.0, \"FIXED_VERT_GAIN\": 12, \"BANDWIDTH_LIMIT\": 1, \"VERTICAL_VERNIER\": 1.0, \"ACQ_VERT_OFFSET\": 0.03420000150799751, \"WAVE_SOURCE\": 0}'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict2save['H1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af1711d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "86b2f7d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m f\u001b[38;5;241m=\u001b[39m\u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprova.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m f\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipped\u001b[39m\u001b[38;5;124m\"\u001b[39m, dict2save[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH1\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m'\u001b[39m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/files.py:241\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    239\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate(name, h5f\u001b[38;5;241m.\u001b[39mACC_EXCL, fapl\u001b[38;5;241m=\u001b[39mfapl, fcpl\u001b[38;5;241m=\u001b[39mfcpl)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:122\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "f=h5py.File('prova.h5', 'w')\n",
    "f.create_dataset(\"zipped\", dict2save['H1'], dtype='string', compression=\"gzip\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff046904",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = pickle.dumps(data)\n",
    "record_data = {'c1': binary_data}\n",
    "len(binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cf01df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: b'\\x80\\x04\\x95(\\x05\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x08wavedesc\\x94}\\x94(\\x8c\\x0fDESCRIPTOR_NAME\\x94\\x8c\\x08WAVEDESC\\x94\\x8c\\rTEMPLATE_NAME\\x94\\x8c\\nLECROY_2_3\\x94\\x8c\\tCOMM_TYPE\\x94K\\x01\\x8c\\nCOMM_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m group \u001b[38;5;241m=\u001b[39m hdf_file\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m record_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbyte\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/dataset.py:46\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Convert data to a C-contiguous ndarray\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty):\n\u001b[0;32m---> 46\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43marray_for_new_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecified_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Validate shape\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/base.py:118\u001b[0m, in \u001b[0;36marray_for_new_object\u001b[0;34m(data, specified_dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     as_dtype \u001b[38;5;241m=\u001b[39m guess_dtype(data)\n\u001b[0;32m--> 118\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mas_dtype)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# In most cases, this does nothing. But if data was already an array,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# and as_dtype is a tagged h5py dtype (e.g. for an object array of strings),\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# asarray() doesn't replace its dtype object. This gives it the tagged dtype:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: b'\\x80\\x04\\x95(\\x05\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x08wavedesc\\x94}\\x94(\\x8c\\x0fDESCRIPTOR_NAME\\x94\\x8c\\x08WAVEDESC\\x94\\x8c\\rTEMPLATE_NAME\\x94\\x8c\\nLECROY_2_3\\x94\\x8c\\tCOMM_TYPE\\x94K\\x01\\x8c\\nCOMM_"
     ]
    }
   ],
   "source": [
    "#append_record_to_hdf5('/Volumes/WC/data/test2.h5', 1, binary_data)\n",
    "\n",
    "with h5py.File('/Volumes/WC/data/test2.h5', 'a') as hdf_file:\n",
    "    group = hdf_file.create_group(str(7))\n",
    "    for key, value in record_data.items():\n",
    "        group.create_dataset(key, data=binary_data, dtype='byte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "09dc0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "60792c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "20579452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/site-packages (3.10.0)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/site-packages (from h5py) (1.26.4)\n",
      "Downloading h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: h5py\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "Successfully installed h5py-3.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "97a124b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with open(filepath, 'rb') as f_in:\n",
    "    with gzip.open(filepath+'.gz', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "05e40d6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object dtype dtype('O') has no native HDF5 equivalent",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m bio \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(bio, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/group.py:483\u001b[0m, in \u001b[0;36mGroup.__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    480\u001b[0m     htype\u001b[38;5;241m.\u001b[39mcommit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     h5o\u001b[38;5;241m.\u001b[39mlink(ds\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, name, lcpl\u001b[38;5;241m=\u001b[39mlcpl)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/h5py/_hl/dataset.py:86\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[0;32m---> 86\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/h5t.pyx:1658\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1682\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:1742\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object dtype dtype('O') has no native HDF5 equivalent"
     ]
    }
   ],
   "source": [
    "import io\n",
    "bio = io.BytesIO()\n",
    "with h5py.File(bio, 'w') as f:\n",
    "    f['dataset'] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba398e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
